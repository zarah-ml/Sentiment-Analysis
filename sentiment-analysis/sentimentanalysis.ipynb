{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "arctic-candy",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with Sagemaker \n",
    "  <i> <b> using BlazingText supervised algorithm </b></i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amber-darwin",
   "metadata": {},
   "source": [
    "## Contents\n",
    "1. Introduction\n",
    "2. Setup  \n",
    "     A. Fetching the Dataset  \n",
    "     B. Exploring the data\n",
    "     C. Data Ingestion \n",
    "3. Training the blazingtext supervised model\n",
    "4. Deploying the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "directed-tunnel",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook demonstrates the use of Amazon SageMaker BlazingText algorithm supervised model for sentiment analysis. BlazingText algorithm provides highly optimized implementations of the Word2vec and text classification algorithms. BlazingText's implementation of the supervised multi-class, multi-label text classification algorithm extends the fastText text classifier to use GPU acceleration with custom CUDA. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "devoted-visibility",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This notebook was created and tested on an ml.t2.medium notebook instance and was run with the Python 3 (Data Science) kernel.\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "   1. The S3 bucket and prefix that you want to use for training and model data. This should be within the same region as the Notebook Instance, training, and hosting.  \n",
    "   2. The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these. Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the boto regexp with a the appropriate full IAM role arn string(s).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gothic-orange",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "prefix = 'sentiment-analysis/supervised'\n",
    "\n",
    "print(region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nonprofit-titanium",
   "metadata": {},
   "source": [
    "### Fetching the Dataset\n",
    "\n",
    "We use the Amazon product reviews dataset. The data provided is actually not in correct json format readable for python. Each row is dictionary but for it to be a valid json format, a square bracket should be at the start and end of the file with , being added at end of each row. \n",
    "\n",
    "1. Download the dataset\n",
    "2. Convert data into correct JSON format\n",
    "3. Convert JSON to CSV format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "congressional-yeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "!wget http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Toys_and_Games_5.json.gz    \n",
    "!gzip -d reviews_Toys_and_Games_5.json.gz\n",
    "\n",
    "# read the entire file into a python array\n",
    "with open(\"reviews_Toys_and_Games_5.json\",'r') as f:\n",
    "    data = f.readlines()\n",
    "    \n",
    "# remove the trailing \"\\n\" from each line  \n",
    "data = map(lambda x: x.rstrip(), data)\n",
    "\n",
    "data_json_str = \"[\" + ','.join(data) + \"]\"\n",
    "\n",
    "#write in another JSON file\n",
    "with open(\"data_json_reviews.json\",'w')as f:\n",
    "    f.write(data_json_str)\n",
    "\n",
    "#load in to Pandas\n",
    "data_df=pd.read_json(\"data_json_reviews.json\")\n",
    "\n",
    "# Convert JSON to CSV File \n",
    "data_df.to_csv('output_reviews_top.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prepared-reception",
   "metadata": {},
   "source": [
    "### Exploring the data\n",
    "\n",
    "The sentiment data is not present in the dataset and therefore sentiment information will be derived from the information in the rating column. Once the sentiment information is added, its distribution is plotted using matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "muslim-louisiana",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv=pd.read_csv('output_reviews_top.csv')\n",
    "print(data_csv.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-estate",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "print(\"Number of rows per star rating:\")\n",
    "print(data_df['overall'].value_counts())\n",
    "\n",
    "# Function to map stars to sentiment\n",
    "def map_sentiment(stars_received):\n",
    "    if stars_received <= 2:\n",
    "        return -1\n",
    "    elif stars_received == 3:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "# Mapping stars to sentiment into three categories\n",
    "data_df['sentiment'] = [ map_sentiment(x) for x in data_df['overall']]\n",
    "\n",
    "# Plotting the sentiment distribution\n",
    "plt.figure()\n",
    "pd.value_counts(data_df['sentiment']).plot.bar(title=\"Sentiment distribution in df\")\n",
    "plt.xlabel(\"Sentiment\")\n",
    "plt.ylabel(\"No. of rows in df\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widespread-minutes",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve top few number of each category\n",
    "def get_top_data(top_n = 5000):\n",
    "    data_df_positive = data_df[data_df['sentiment'] == 1].head(top_n)\n",
    "    data_df_negative = data_df[data_df['sentiment'] == -1].head(top_n)\n",
    "    data_df_neutral = data_df[data_df['sentiment'] == 0].head(top_n)\n",
    "    data_df_small = pd.concat([data_df_positive, data_df_negative, data_df_neutral])\n",
    "    return data_df_small\n",
    "\n",
    "# Function call to get the top 10000 from each sentiment\n",
    "top_data_df_small = get_top_data(top_n=10000)\n",
    "\n",
    "# After selecting top few samples of each sentiment\n",
    "print(\"After segregating and taking equal number of rows for each sentiment:\")\n",
    "print(top_data_df_small['sentiment'].value_counts())\n",
    "top_data_df_small.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interesting-communications",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove irrelevant information  \n",
    "sparse_df = top_data_df_small.drop(['reviewerID','asin','reviewerName','helpful','overall','unixReviewTime','reviewTime'],axis=1)\n",
    "sparse_df = sparse_df.reindex(columns=['sentiment','reviewText','summary'])\n",
    "print(sparse_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respective-vienna",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach label to each Sentiment \n",
    "index_to_label ={'1':'Positive','0':'neutral','-1':'Negative'}\n",
    "print(index_to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entire-toolbox",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle data and split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_path = 'train.csv'\n",
    "test_path = 'test.csv'\n",
    "\n",
    "train, test = train_test_split(sparse_df, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "train.to_csv(train_path)\n",
    "test.to_csv(test_path)\n",
    "\n",
    "print(train.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trying-watson",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "import csv\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def transform_instance(row):\n",
    "    cur_row = []\n",
    "    label = \"__label__\" + index_to_label[row[1]]  #Prefix the index-ed label with __label__\n",
    "    cur_row.append(label)\n",
    "    cur_row.extend(nltk.word_tokenize(row[2].lower()))\n",
    "    cur_row.extend(nltk.word_tokenize(row[3].lower()))\n",
    "    return cur_row\n",
    "\n",
    "\n",
    "def preprocess(input_file, output_file, keep=1):\n",
    "    all_rows = []\n",
    "    with open(input_file, 'r') as csvinfile:\n",
    "        csv_reader = csv.reader(csvinfile, delimiter=',')\n",
    "        next(csv_reader)\n",
    "        for row in csv_reader: \n",
    "            all_rows.append(row)\n",
    "    shuffle(all_rows)\n",
    "    all_rows = all_rows[:int(keep*len(all_rows))]\n",
    "    pool = Pool(processes=multiprocessing.cpu_count())\n",
    "    transformed_rows = pool.map(transform_instance, all_rows)\n",
    "    pool.close() \n",
    "    pool.join()\n",
    "    \n",
    "    with open(output_file, 'w') as csvoutfile:\n",
    "        csv_writer = csv.writer(csvoutfile, delimiter=' ', lineterminator='\\n')\n",
    "        csv_writer.writerows(transformed_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "small-essex",
   "metadata": {},
   "source": [
    "### Data Ingestion\n",
    "\n",
    "Next, we read the dataset from the existing repository into memory, for preprocessing prior to training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-agency",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Preparing the training dataset\n",
    "preprocess('train.csv', 'sentiment.train')\n",
    "        \n",
    "# Preparing the validation dataset        \n",
    "preprocess('test.csv', 'sentiment.validation')\n",
    "\n",
    "# create train and validation channels \n",
    "train_channel = prefix + '/train'\n",
    "validation_channel = prefix + '/validation'\n",
    "\n",
    "sess.upload_data('sentiment.train', bucket = bucket, key_prefix = train_channel )\n",
    "sess.upload_data('sentiment.validation', bucket = bucket, key_prefix = validation_channel )\n",
    "\n",
    "s3_train_data = 's3://{}/{}'.format(bucket,train_channel)\n",
    "s3_validation_data = 's3://{}/{}'.format(bucket,validation_channel)\n",
    "s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "united-encounter",
   "metadata": {},
   "source": [
    "## Training the BlazingText supervised model\n",
    "\n",
    "Now that we have prepared the dataset, we are ready to train the model. We first initiate an estimator which includes:\n",
    "\n",
    "   * The container image for the algorithm (blazingtext)\n",
    "   * Configuration for the output of the training jobs\n",
    "   * The values of static algorithm hyperparameters, those that are not specified will be given default values\n",
    "   * The type and number of instances to use for the training jobs\n",
    " \n",
    "Then we create data channels to read from the S3 source and then fit the model on the training and validation data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fantastic-stanford",
   "metadata": {},
   "outputs": [],
   "source": [
    "container = sagemaker.amazon.amazon_estimator.get_image_uri(region, \"blazingtext\", \"latest\")\n",
    "print('Using SageMaker BlazingText container: {} ({})'.format(container, region))\n",
    "\n",
    "sa_model = sagemaker.estimator.Estimator(container,\n",
    "                                         role, \n",
    "                                         instance_count=1, \n",
    "                                         instance_type='ml.c4.4xlarge',\n",
    "                                         volume_size = 30,\n",
    "                                         max_run = 360000,\n",
    "                                         input_mode= 'File',\n",
    "                                         output_path=s3_output_location,\n",
    "                                         sagemaker_session=sess)\n",
    "\n",
    "sa_model.set_hyperparameters(mode='supervised',\n",
    "                            epochs=1,\n",
    "                            min_count=2,\n",
    "                            learning_rate= 0.05,\n",
    "                            vector_dim=10,\n",
    "                            early_stopping=True,\n",
    "                            patience=4,\n",
    "                            min_epochs=5,\n",
    "                            word_ngrams=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spectacular-blogger",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sagemaker.inputs.TrainingInput(s3_train_data, distribution='FullyReplicated', \n",
    "                        content_type='text/plain', s3_data_type='S3Prefix')\n",
    "validation_data = sagemaker.inputs.TrainingInput(s3_validation_data, distribution='FullyReplicated', \n",
    "                             content_type='text/plain', s3_data_type='S3Prefix')\n",
    "data_channels = {'train': train_data, 'validation': validation_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aging-tyler",
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_model.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "german-investigator",
   "metadata": {},
   "source": [
    "## Deploying the endpoint for testing\n",
    "\n",
    "Testing the trained model on sample sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-terrain",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "\n",
    "text_classifier = sa_model.deploy(\n",
    "    initial_instance_count = 1,\n",
    "    instance_type = 'ml.m4.xlarge',\n",
    "    serializer=JSONSerializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "looking-status",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"Great product to have.\",\n",
    "            \"unsatisfied.\"]\n",
    "\n",
    "# using the same nltk tokenizer that we used during data preparation for training\n",
    "tokenized_sentences = [' '.join(nltk.word_tokenize(sent)) for sent in sentences]\n",
    "\n",
    "payload = {\"instances\" : tokenized_sentences}\n",
    "\n",
    "response = text_classifier.predict(payload)\n",
    "\n",
    "predictions = json.loads(response)\n",
    "print(json.dumps(predictions, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-advance",
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_model.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
